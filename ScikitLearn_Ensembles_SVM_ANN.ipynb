{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3 : SVMs, Neural Nets, Ensembles\n",
    "\n",
    "In this assignment you'll implement SVMs, Neural Nets, and Ensembling methods to classify patients as either having or not having diabetic retinopathy. For this task we'll be using the same Diabetic Retinopathy data set which was used in the previous assignments. You can find additional details about the dataset [here](http://archive.ics.uci.edu/ml/datasets/Diabetic+Retinopathy+Debrecen+Data+Set). You'll explore how to train SVMs, NNs, and Ensembles using the `scikit-learn` library. The scikit-learn documentation can be found [here](http://scikit-learn.org/stable/documentation.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#You may add additional imports\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1151, 20)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>quality</th>\n",
       "      <th>prescreen</th>\n",
       "      <th>ma2</th>\n",
       "      <th>ma3</th>\n",
       "      <th>ma4</th>\n",
       "      <th>ma5</th>\n",
       "      <th>ma6</th>\n",
       "      <th>ma7</th>\n",
       "      <th>exudate8</th>\n",
       "      <th>exudate9</th>\n",
       "      <th>exudate10</th>\n",
       "      <th>exudate11</th>\n",
       "      <th>exudate12</th>\n",
       "      <th>exudate13</th>\n",
       "      <th>exudate14</th>\n",
       "      <th>exudate15</th>\n",
       "      <th>euDist</th>\n",
       "      <th>diameter</th>\n",
       "      <th>amfm_class</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>19</td>\n",
       "      <td>18</td>\n",
       "      <td>14</td>\n",
       "      <td>49.895756</td>\n",
       "      <td>17.775994</td>\n",
       "      <td>5.270920</td>\n",
       "      <td>0.771761</td>\n",
       "      <td>0.018632</td>\n",
       "      <td>0.006864</td>\n",
       "      <td>0.003923</td>\n",
       "      <td>0.003923</td>\n",
       "      <td>0.486903</td>\n",
       "      <td>0.100025</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>22</td>\n",
       "      <td>18</td>\n",
       "      <td>16</td>\n",
       "      <td>13</td>\n",
       "      <td>57.709936</td>\n",
       "      <td>23.799994</td>\n",
       "      <td>3.325423</td>\n",
       "      <td>0.234185</td>\n",
       "      <td>0.003903</td>\n",
       "      <td>0.003903</td>\n",
       "      <td>0.003903</td>\n",
       "      <td>0.003903</td>\n",
       "      <td>0.520908</td>\n",
       "      <td>0.144414</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "      <td>60</td>\n",
       "      <td>59</td>\n",
       "      <td>54</td>\n",
       "      <td>47</td>\n",
       "      <td>33</td>\n",
       "      <td>55.831441</td>\n",
       "      <td>27.993933</td>\n",
       "      <td>12.687485</td>\n",
       "      <td>4.852282</td>\n",
       "      <td>1.393889</td>\n",
       "      <td>0.373252</td>\n",
       "      <td>0.041817</td>\n",
       "      <td>0.007744</td>\n",
       "      <td>0.530904</td>\n",
       "      <td>0.128548</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>55</td>\n",
       "      <td>53</td>\n",
       "      <td>53</td>\n",
       "      <td>50</td>\n",
       "      <td>43</td>\n",
       "      <td>31</td>\n",
       "      <td>40.467228</td>\n",
       "      <td>18.445954</td>\n",
       "      <td>9.118901</td>\n",
       "      <td>3.079428</td>\n",
       "      <td>0.840261</td>\n",
       "      <td>0.272434</td>\n",
       "      <td>0.007653</td>\n",
       "      <td>0.001531</td>\n",
       "      <td>0.483284</td>\n",
       "      <td>0.114790</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>44</td>\n",
       "      <td>44</td>\n",
       "      <td>44</td>\n",
       "      <td>41</td>\n",
       "      <td>39</td>\n",
       "      <td>27</td>\n",
       "      <td>18.026254</td>\n",
       "      <td>8.570709</td>\n",
       "      <td>0.410381</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.475935</td>\n",
       "      <td>0.123572</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>44</td>\n",
       "      <td>43</td>\n",
       "      <td>41</td>\n",
       "      <td>41</td>\n",
       "      <td>37</td>\n",
       "      <td>29</td>\n",
       "      <td>28.356400</td>\n",
       "      <td>6.935636</td>\n",
       "      <td>2.305771</td>\n",
       "      <td>0.323724</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.502831</td>\n",
       "      <td>0.126741</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "      <td>27</td>\n",
       "      <td>25</td>\n",
       "      <td>16</td>\n",
       "      <td>15.448398</td>\n",
       "      <td>9.113819</td>\n",
       "      <td>1.633493</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.541743</td>\n",
       "      <td>0.139575</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>20.679649</td>\n",
       "      <td>9.497786</td>\n",
       "      <td>1.223660</td>\n",
       "      <td>0.150382</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.576318</td>\n",
       "      <td>0.071071</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>21</td>\n",
       "      <td>18</td>\n",
       "      <td>15</td>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "      <td>66.691933</td>\n",
       "      <td>23.545543</td>\n",
       "      <td>6.151117</td>\n",
       "      <td>0.496372</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500073</td>\n",
       "      <td>0.116793</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>79</td>\n",
       "      <td>75</td>\n",
       "      <td>73</td>\n",
       "      <td>71</td>\n",
       "      <td>64</td>\n",
       "      <td>47</td>\n",
       "      <td>22.141784</td>\n",
       "      <td>10.054384</td>\n",
       "      <td>0.874633</td>\n",
       "      <td>0.099780</td>\n",
       "      <td>0.023386</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.560959</td>\n",
       "      <td>0.109134</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   quality  prescreen  ma2  ma3  ma4  ma5  ma6  ma7   exudate8   exudate9  \\\n",
       "0        1          1   22   22   22   19   18   14  49.895756  17.775994   \n",
       "1        1          1   24   24   22   18   16   13  57.709936  23.799994   \n",
       "2        1          1   62   60   59   54   47   33  55.831441  27.993933   \n",
       "3        1          1   55   53   53   50   43   31  40.467228  18.445954   \n",
       "4        1          1   44   44   44   41   39   27  18.026254   8.570709   \n",
       "5        1          1   44   43   41   41   37   29  28.356400   6.935636   \n",
       "6        1          0   29   29   29   27   25   16  15.448398   9.113819   \n",
       "7        1          1    6    6    6    6    2    1  20.679649   9.497786   \n",
       "8        1          1   22   21   18   15   13   10  66.691933  23.545543   \n",
       "9        1          1   79   75   73   71   64   47  22.141784  10.054384   \n",
       "\n",
       "   exudate10  exudate11  exudate12  exudate13  exudate14  exudate15    euDist  \\\n",
       "0   5.270920   0.771761   0.018632   0.006864   0.003923   0.003923  0.486903   \n",
       "1   3.325423   0.234185   0.003903   0.003903   0.003903   0.003903  0.520908   \n",
       "2  12.687485   4.852282   1.393889   0.373252   0.041817   0.007744  0.530904   \n",
       "3   9.118901   3.079428   0.840261   0.272434   0.007653   0.001531  0.483284   \n",
       "4   0.410381   0.000000   0.000000   0.000000   0.000000   0.000000  0.475935   \n",
       "5   2.305771   0.323724   0.000000   0.000000   0.000000   0.000000  0.502831   \n",
       "6   1.633493   0.000000   0.000000   0.000000   0.000000   0.000000  0.541743   \n",
       "7   1.223660   0.150382   0.000000   0.000000   0.000000   0.000000  0.576318   \n",
       "8   6.151117   0.496372   0.000000   0.000000   0.000000   0.000000  0.500073   \n",
       "9   0.874633   0.099780   0.023386   0.000000   0.000000   0.000000  0.560959   \n",
       "\n",
       "   diameter  amfm_class  label  \n",
       "0  0.100025           1      0  \n",
       "1  0.144414           0      0  \n",
       "2  0.128548           0      1  \n",
       "3  0.114790           0      0  \n",
       "4  0.123572           0      1  \n",
       "5  0.126741           0      1  \n",
       "6  0.139575           0      1  \n",
       "7  0.071071           1      0  \n",
       "8  0.116793           0      1  \n",
       "9  0.109134           0      1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the data from csv file\n",
    "col_names = []\n",
    "for i in range(20):\n",
    "    if i == 0:\n",
    "        col_names.append('quality')\n",
    "    if i == 1:\n",
    "        col_names.append('prescreen')\n",
    "    if i >= 2 and i <= 7:\n",
    "        col_names.append('ma' + str(i))\n",
    "    if i >= 8 and i <= 15:\n",
    "        col_names.append('exudate' + str(i))\n",
    "    if i == 16:\n",
    "        col_names.append('euDist')\n",
    "    if i == 17:\n",
    "        col_names.append('diameter')\n",
    "    if i == 18:\n",
    "        col_names.append('amfm_class')\n",
    "    if i == 19:\n",
    "        col_names.append('label')\n",
    "\n",
    "data = pd.read_csv(\"messidor_features.txt\", names = col_names)\n",
    "print(data.shape)\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. Separate the feature columns from the class label column. You should end up with two separate data frames - one that contains all of the feature values and one that contains the class labels.\n",
    "\n",
    "For some classification algorithms, like SVMs and Neural Nets, scaling of the data is critical for the algorithm to operate correctly. For other classification algorithms, data scaling is not necessary (like Naive Bayes and Decision Trees). But using scaled data with an algorithm that doesn't explicitly need it to be scaled does not hurt the results of that algorithm. So we will go ahead and scale the data and *use the scaled data going forward*. \n",
    "\n",
    "Use `sklearn.preprocessing.StandardScaler` to standardize the dataset’s features (mean = 0 and variance = 1). Only standardize the the features, not the class labels! Note that StandardScaler returns a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1151,)\n",
      "(1151, 19)\n"
     ]
    }
   ],
   "source": [
    "# Separate data set into values and labels\n",
    "data_header = data.pop('label')\n",
    "data_values = data.values\n",
    "print(data_header.shape)\n",
    "print(data_values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1151, 19)\n"
     ]
    }
   ],
   "source": [
    "#scaling the data_values\n",
    "#http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(data_values)\n",
    "data_standardized = scaler.transform(data_values)\n",
    "print(data_std.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Support Vector Machines (SVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2. Create an `sklearn.svm.SVC` (Support Vector Classifier) to classify the data. Use `sklearn.model_selection.GridSearchCV` to find the best kernel for this dataset. Try the kernels: `linear`, `rbf` (radial basis kernel), `poly` (polynomial), and `sigmoid`. Use a 5-fold cross validation and print out the best kernel (`best_params_`) and best accuracy achieved with this kernel (`best_score_`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best kernel: linear\n",
      "Best Accuracy: 0.7228496959165943\n"
     ]
    }
   ],
   "source": [
    "# Create Support Vector Classifier\n",
    "#http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "svc = SVC()\n",
    "parameters = {'kernel': ('linear', 'rbf', 'poly','sigmoid')}\n",
    "clf = GridSearchCV(svc, parameters, cv=5)\n",
    "clf.fit(data_standardized, data_header)\n",
    "#svc.get_params().keys()\n",
    "print('Best kernel:', clf.best_params_['kernel'])\n",
    "print('Best Accuracy:', clf.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3. Create a new `sklearn.svm.SVC` using the best kernel that was found in Q2. Use `sklearn.model_selection.GridSearchCV` to find the best value of C for this SVM. Try values from 1-250 by increments of 10 (you can use the `range` funtion to do this). Use a 5-fold cross validation and print out the best value of C (`best_params_`) and best accuracy achieved with this value of C (`best_score_`).\n",
    "\n",
    "Be patient, as this can take a few minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'C': [1, 11, 21, 31, 41, 51, 61, 71, 81, 91, 101, 111, 121, 131, 141, 151, 161, 171, 181, 191, 201, 211, 221, 231, 241]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create new SVC using the best kernel --> linear.\n",
    "svc2 = SVC(kernel='linear')\n",
    "#Use grdsearchCV to find best value of C for this SVM.\n",
    "parameters2 = {'C':list(range(1,250,10))}\n",
    "clf2 = GridSearchCV(svc2, parameters2, cv=5)\n",
    "clf2.fit(data_standardized, data_header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best C value: 221\n",
      "Best Accuracy: 0.7463075586446568\n"
     ]
    }
   ],
   "source": [
    "print('Best C value:', clf2.best_params_['C'])\n",
    "print('Best Accuracy:', clf2.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Neural Networks (NN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4. Train a multi-layer perceptron with a single hidden layer using `sklearn.neural_network.MLPClassifier`. \n",
    "* Use `GridSearchCV` with 5 fold cross validation to find the best hidden layer size and the best activation function. \n",
    "* Try values of `hidden_layer_sizes` ranging from `(10,)` to `(60,)` with gaps of 10.\n",
    "* Try activation functions `logistic`, `tanh`, `relu`.\n",
    "\n",
    "Wrap your `GridSearchCV` in a 5-fold `cross_val_score` and report the accuracy of your neural net.\n",
    "\n",
    "Be patient, as this can take a few minutes to run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create MLP classifier.\n",
    "#http://scikit-learn.org/stable/modules/neural_networks_supervised.html\n",
    "#http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp = MLPClassifier()\n",
    "parameters3 = {'hidden_layer_sizes':list(range(10, 60, 10)), 'activation':('logistic', 'tanh', 'relu')}\n",
    "clf3 = GridSearchCV(mlp, parameters3, cv=5)\n",
    "clf3 = clf3.fit(data_standardized, data_header)\n",
    "result_mlp = cross_val_score(clf3, data_standardized, data_header, cv=5 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden Layer Size: 50\n",
      "Activation: tanh\n",
      "Best Accuracy: 0.7280594767551289\n"
     ]
    }
   ],
   "source": [
    "print('Hidden Layer Size:', clf3.best_params_['hidden_layer_sizes'] )\n",
    "print('Activation:', clf3.best_params_['activation'] )\n",
    "print('Best Accuracy:',result_mlp.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Ensemble Classifiers\n",
    "\n",
    "Ensemble classifiers combine the predictions of multiple base estimators to improve the accuracy of the predictions. One of the key assumptions that ensemble classifiers make is that the base estimators are built independently (so they are diverse)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A. Random Forests**\n",
    "\n",
    "Q5. Use `sklearn.ensemble.RandomForestClassifier` to classify the data. Use a `GridSearchCV` to tune the hyperparameters to get the best results. \n",
    "* Try `max_depth` ranging from 35-55\n",
    "* Try `min_samples_leaf` of 8, 10, 12\n",
    "* Try `max_features` of `\"sqrt\"` and `\"log2\"`\n",
    "\n",
    "Wrap your GridSearchCV in a cross_val_score with 5-fold CV to report the accuracy of the model.\n",
    "\n",
    "Be patient, this can take a few minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'max_depth': [35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54], 'min_samples_leaf': (8, 10, 12), 'max_features': ('sqrt', 'log2')},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create RandomForestClassifier\n",
    "#http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc = RandomForestClassifier()\n",
    "parameters4 = {'max_depth': list(range(35,55)), 'min_samples_leaf': (8,10,12), 'max_features':('sqrt', 'log2')}\n",
    "clf4 = GridSearchCV( rfc, parameters4, cv=5)\n",
    "clf4.fit(data_standardized, data_header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max depth: 49\n",
      "Min Samples Lead: 10\n",
      "Max Features: sqrt\n",
      "Best Accuracy: 0.6646226237530585\n"
     ]
    }
   ],
   "source": [
    "#Wrap in a cross_val_score with cv=5\n",
    "result_rfc = cross_val_score(clf4, data_standardized, data_header, cv=5 )\n",
    "print('Max depth:' ,clf4.best_params_['max_depth'])\n",
    "print('Min Samples Lead:', clf4.best_params_['min_samples_leaf'])\n",
    "print('Max Features:', clf4.best_params_['max_features'])\n",
    "print('Best Accuracy:', result_rfc.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B. AdaBoost**\n",
    "\n",
    "Random Forests are a kind of averaging ensemble classifier, where the driving principle is to build several estimators independently and then to average their predictions (by taking a vote). In contrast, there is another class of training ensemble classifiers called *boosting* methods. Here the classifiers are trained one-by-one and each time the sampling of the training set depends on the performance of previously generated models.\n",
    "\n",
    "Q6. Evaluate a `sklearn.ensemble.AdaBoostClassifier` classifier on the data. By default, `AdaBoostClassifier` uses decision trees as the base classifiers (but this can be changed). Use 150 base classifiers to make an `AdaBoostClassifier` and evaluate it's accuracy with a 5-fold-CV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.6976510446075663\n"
     ]
    }
   ],
   "source": [
    "# Creating the AdaBoost Classifier\n",
    "#http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "adab = AdaBoostClassifier(n_estimators=150)\n",
    "result_adab = cross_val_score(adab, data_standardized, data_header, cv=5)\n",
    "print('Accuracy', result_adab.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Deploying a final model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Over the course of three programming assignments, you have tested all kinds of classifiers on this data. Some have performed better than others. \n",
    "\n",
    "We probably wouldn't want to deploy any of these models in the real world to actually diagnose patients because the accuracies are not very high. We could try to improve the accuracy of our models by tweaking their parameters more (testing more hyperparameters using GridSearchCV) and/or we could do some feature engineering on our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q7. Let's say we *did* get to the point where we had a model with very high accuracy and we want to deploy that model and use it for real-world predictions.\n",
    "\n",
    "* Let's say we're going to deploy our neural net classifier.\n",
    "* We need to make one final version of this model, where we use ALL of our available data for training (we do not hold out a test set this time, so no outer cross-validation loop). \n",
    "* We need to tune the parameters of the model on the FULL dataset, so copy the code you entered for Q4, but remove the outer cross validation loop (remove `cross_val_score`). Just run the `GridSearchCV` by calling `fit` on it and passing in the full dataset. This results in the final trained model with the best parameters for the full dataset. You can print out `best_params_` to see what they are.\n",
    "* The accuracy of this model is what you assessed and reported in Q4.\n",
    "\n",
    "\n",
    "* Use the `pickle` package to save your model. We have provided the lines of code for you, just make sure your final model gets passed in to `pickle.dump()`. This will save your model to a file called finalized_model.sav in your current working directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden Layer Size: 50\n",
      "Activation: relu\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# your code goes here\n",
    "mlp = MLPClassifier()\n",
    "parameters5 = {'hidden_layer_sizes':list(range(10, 60, 10)), 'activation':('logistic', 'tanh', 'relu')}\n",
    "clf5 = GridSearchCV(mlp, parameters5)\n",
    "\n",
    "\n",
    "#replace this final_model with your final model\n",
    "final_model = clf5.fit(data_standardized, data_header)\n",
    "\n",
    "print('Hidden Layer Size:', clf5.best_params_['hidden_layer_sizes'] )\n",
    "print('Activation:', clf5.best_params_['activation'] )\n",
    "\n",
    "filename = 'finalized_model.sav'\n",
    "pickle.dump(final_model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q8. Now if someone wants to use your trained, saved classifier to classify a new record, they can load the saved model and just call predict on it. \n",
    "* Given this new record, classify it with your saved model and print out either \"Negative for disease\" or \"Positive for disease.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV(cv=None, error_score='raise',\n",
      "       estimator=MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=(100,), learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
      "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
      "       verbose=False, warm_start=False),\n",
      "       fit_params=None, iid=True, n_jobs=1,\n",
      "       param_grid={'hidden_layer_sizes': [10, 20, 30, 40, 50], 'activation': ('logistic', 'tanh', 'relu')},\n",
      "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
      "       scoring=None, verbose=0)\n",
      "predicted: [1]\n"
     ]
    }
   ],
   "source": [
    "# some time later...\n",
    "\n",
    "# use this as the new record to classify\n",
    "record = [[ 0.05905386, 0.2982129, 0.68613149, 0.75078865, 0.87119216, 0.88615694,\n",
    "  0.93600623, 0.98369184, -0.47426472, -0.57642756, -0.53115361, -0.42789774,\n",
    " -0.21907738, -0.20090532, -0.21496782, -0.2080998, 0.06692373, -2.81681183,\n",
    " -0.7117194 ]]\n",
    "\n",
    " \n",
    "# load the model from disk\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "\n",
    "# your code goes here\n",
    "print(loaded_model)\n",
    "pred = loaded_model.predict(record)\n",
    "print('predicted:', pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Given the predicted result is 1, the new record is positive for disease.'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
